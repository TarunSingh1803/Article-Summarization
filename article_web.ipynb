{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zeq7JblzOKl8"
      },
      "outputs": [],
      "source": [
        "# main.py\n",
        "# This script provides the core functionality for an Article Summarization and Link Analysis Platform.\n",
        "# It uses pre-trained models for summarization and established libraries for web scraping and NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XbdD2pu0PV2b"
      },
      "outputs": [],
      "source": [
        "# !pip install rouge-score\n",
        "# !pip install yake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw7ooPWAOPdy",
        "outputId": "4b08babc-396d-4472-ffa4-16a807c7497b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Tarun Singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\Tarun\n",
            "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\Tarun\n",
            "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- 1. Installation ---\n",
        "# Before running, make sure you have all the necessary libraries installed.\n",
        "# You can install them using pip:\n",
        "# pip install torch transformers beautifulsoup4 requests scikit-learn rouge-score yake nltk\n",
        "\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge_score import rouge_scorer\n",
        "import yake\n",
        "import nltk\n",
        "\n",
        "# NLTK's sentence tokenizer is needed for processing text.\n",
        "# The first time you run this, it will download the necessary data.\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "veJ45UIgOPaS"
      },
      "outputs": [],
      "source": [
        "# --- 2. Model and Tokenizer Initialization ---\n",
        "# We use a pre-trained BART model from Hugging Face, which is excellent for summarization.\n",
        "# This avoids the need for training a model from scratch, which is very resource-intensive.\n",
        "def initialize_model():\n",
        "    \"\"\"\n",
        "    Loads and initializes the BART model and tokenizer from Hugging Face.\n",
        "    \"\"\"\n",
        "    print(\"Initializing the summarization model...\")\n",
        "    model_name = 'facebook/bart-large-cnn'\n",
        "    # The tokenizer prepares the text for the model.\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    # The model itself.\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    print(\"Model initialized successfully.\")\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GlxOs8BJOPWk"
      },
      "outputs": [],
      "source": [
        "# --- 3. Article Scraping ---\n",
        "def get_article_text(url):\n",
        "    \"\"\"\n",
        "    Fetches and extracts the main text content from a given news article URL.\n",
        "    It focuses on extracting text from <p> (paragraph) tags.\n",
        "    \"\"\"\n",
        "    print(f\"Fetching article from: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find all paragraph tags and join their text.\n",
        "        # This is a simple approach and might need adjustment for different website structures.\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        if not article_text:\n",
        "            print(\"Warning: No text could be extracted from <p> tags. The page might be structured differently.\")\n",
        "            return None\n",
        "\n",
        "        print(\"Article text extracted successfully.\")\n",
        "        return article_text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the URL: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F6LtNOKWOa2R"
      },
      "outputs": [],
      "source": [
        "# --- 4. Text Summarization ---\n",
        "def summarize_text(tokenizer, model, text, max_summary_length=150, min_summary_length=50):\n",
        "    \"\"\"\n",
        "    Generates a summary for the given text using the initialized BART model.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"No text provided for summarization.\"\n",
        "\n",
        "    print(\"Generating summary...\")\n",
        "    # Prepare the text for BART. The tokenizer converts the text string into tensor format.\n",
        "    inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "    # Generate the summary. The model predicts the most likely sequence of tokens.\n",
        "    summary_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        num_beams=4, # `num_beams` > 1 uses beam search for higher quality output\n",
        "        max_length=max_summary_length,\n",
        "        min_length=min_summary_length,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated token IDs back into a human-readable string.\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    print(\"Summary generated.\")\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JEbemcd9Oayx"
      },
      "outputs": [],
      "source": [
        "# --- 5. Keyword Extraction ---\n",
        "def extract_keywords(text):\n",
        "    \"\"\"\n",
        "    Extracts the most relevant keywords from the text using the YAKE algorithm.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    print(\"Extracting keywords...\")\n",
        "    # Initialize the YAKE keyword extractor.\n",
        "    # You can customize language, n-gram size, etc.\n",
        "    kw_extractor = yake.KeywordExtractor(top=10, n=2) # Extract top 10 keywords, up to 2-grams\n",
        "    keywords = kw_extractor.extract_keywords(text)\n",
        "\n",
        "    # Return just the keyword text, not the scores.\n",
        "    keyword_list = [kw for kw, score in keywords]\n",
        "    print(f\"Keywords found: {keyword_list}\")\n",
        "    return keyword_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lG_kyjt1Oh3N"
      },
      "outputs": [],
      "source": [
        "# --- 6. Finding Related Articles ---\n",
        "def find_related_articles(target_article_text, articles_db):\n",
        "    \"\"\"\n",
        "    Finds the most similar article to the target article from a database of articles.\n",
        "    This uses TF-IDF to vectorize the text and cosine similarity to measure relatedness.\n",
        "    \"\"\"\n",
        "    if not target_article_text:\n",
        "        return None, None\n",
        "\n",
        "    print(\"Finding related articles...\")\n",
        "    # Create a list of all article texts, with the target article at the end.\n",
        "    all_texts = [article['text'] for article in articles_db]\n",
        "    all_texts.append(target_article_text)\n",
        "\n",
        "    # Initialize the TF-IDF Vectorizer. This will convert text into numerical vectors.\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "    # Calculate the cosine similarity between the target article (last one) and all others.\n",
        "    cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
        "\n",
        "    # Find the index of the most similar article.\n",
        "    most_similar_article_index = cosine_similarities.argmax()\n",
        "    highest_similarity_score = cosine_similarities[most_similar_article_index]\n",
        "\n",
        "    # Get the most similar article's title from our mock database.\n",
        "    related_article = articles_db[most_similar_article_index]\n",
        "\n",
        "    print(f\"Most related article found: '{related_article['title']}' with a similarity score of {highest_similarity_score:.2f}\")\n",
        "    return related_article, highest_similarity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7. ROUGE Score Evaluation ---\n",
        "def evaluate_summary(generated_summary, reference_summary):\n",
        "    \"\"\"\n",
        "    Calculates ROUGE scores to evaluate the quality of the generated summary\n",
        "    against a human-written reference summary.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- ROUGE Evaluation ---\")\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "    print(f\"Generated Summary: {generated_summary}\")\n",
        "    print(f\"Reference Summary: {reference_summary}\")\n",
        "    print(\"\\nScores:\")\n",
        "    for key, score in scores.items():\n",
        "        print(f\"  {key}: Precision={score.precision:.4f}, Recall={score.recall:.4f}, F-measure={score.fmeasure:.4f}\")\n",
        "    print(\"------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RILe-GiXDXj",
        "outputId": "5f6118dc-be10-410a-a4a2-46544b09d3d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing the summarization model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Tarun Singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully.\n",
            "Choose input method:\n",
            "1. Enter a URL to fetch the article\n",
            "2. Paste article text manually\n",
            "Fetching article from: https://www.bbc.com/news/articles/ckgj7jxkq58o\n",
            "Fetching article from: https://www.bbc.com/news/articles/ckgj7jxkq58o\n",
            "Article text extracted successfully.\n",
            "\n",
            "--- Processing Article ---\n",
            "Article Text: 'A US appeals court has ruled that most tariffs issued by US President Donald Trump are illegal, setting up a potential legal showdown that could upend his foreign policy agenda. The ruling affects Trump's so-called \"reciprocal\" tariffs, imposed on most countries around the world, as well as other tariffs slapped on China, Mexico and Canada. In a 7-4 decision, the US Court of Appeals for the Federal Circuit rejected Trump's argument that the tariffs were permitted under an emergency economic powe...'\n",
            "Generating summary...\n",
            "Article text extracted successfully.\n",
            "\n",
            "--- Processing Article ---\n",
            "Article Text: 'A US appeals court has ruled that most tariffs issued by US President Donald Trump are illegal, setting up a potential legal showdown that could upend his foreign policy agenda. The ruling affects Trump's so-called \"reciprocal\" tariffs, imposed on most countries around the world, as well as other tariffs slapped on China, Mexico and Canada. In a 7-4 decision, the US Court of Appeals for the Federal Circuit rejected Trump's argument that the tariffs were permitted under an emergency economic powe...'\n",
            "Generating summary...\n",
            "Summary generated.\n",
            "Extracting keywords...\n",
            "Keywords found: ['tariffs', 'court', 'appeals court', 'Donald Trump', 'Trump', 'President Donald', 'Supreme Court', 'President', 'impose tariffs', 'appeals']\n",
            "Finding related articles...\n",
            "Most related article found: 'Global Markets Rally on Tech Sector Growth' with a similarity score of 0.03\n",
            "\n",
            "\n",
            "=============================================\n",
            "          ANALYSIS COMPLETE\n",
            "=============================================\n",
            "\n",
            "✅ GENERATED SUMMARY:\n",
            "US Court of Appeals for the Federal Circuit rules tariffs are illegal. Ruling will not take effect until 14 October, to give the administration time to ask the US Supreme Court to take up the case. The ruling affects Trump's so-called \"reciprocal\" tariffs, imposed on most countries around the world, as well as other tariffs slapped on China, Mexico and Canada.\n",
            "\n",
            "✅ EXTRACTED KEYWORDS:\n",
            "tariffs, court, appeals court, Donald Trump, Trump, President Donald, Supreme Court, President, impose tariffs, appeals\n",
            "\n",
            "✅ MOST RELATED ARTICLE (from our database):\n",
            "   Title: Global Markets Rally on Tech Sector Growth (Similarity Score: 0.03)\n",
            "=============================================\n",
            "\n",
            "\n",
            "--- ROUGE Evaluation ---\n",
            "Generated Summary: US Court of Appeals for the Federal Circuit rules tariffs are illegal. Ruling will not take effect until 14 October, to give the administration time to ask the US Supreme Court to take up the case. The ruling affects Trump's so-called \"reciprocal\" tariffs, imposed on most countries around the world, as well as other tariffs slapped on China, Mexico and Canada.\n",
            "Reference Summary: A new AI model can analyze medical images more accurately than human radiologists, helping doctors diagnose diseases earlier. This technology, trained on millions of images, is expected to improve patient care and accelerate drug development.\n",
            "\n",
            "Scores:\n",
            "  rouge1: Precision=0.0635, Recall=0.1143, F-measure=0.0816\n",
            "  rouge2: Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
            "  rougeL: Precision=0.0476, Recall=0.0857, F-measure=0.0612\n",
            "------------------------\n",
            "Summary generated.\n",
            "Extracting keywords...\n",
            "Keywords found: ['tariffs', 'court', 'appeals court', 'Donald Trump', 'Trump', 'President Donald', 'Supreme Court', 'President', 'impose tariffs', 'appeals']\n",
            "Finding related articles...\n",
            "Most related article found: 'Global Markets Rally on Tech Sector Growth' with a similarity score of 0.03\n",
            "\n",
            "\n",
            "=============================================\n",
            "          ANALYSIS COMPLETE\n",
            "=============================================\n",
            "\n",
            "✅ GENERATED SUMMARY:\n",
            "US Court of Appeals for the Federal Circuit rules tariffs are illegal. Ruling will not take effect until 14 October, to give the administration time to ask the US Supreme Court to take up the case. The ruling affects Trump's so-called \"reciprocal\" tariffs, imposed on most countries around the world, as well as other tariffs slapped on China, Mexico and Canada.\n",
            "\n",
            "✅ EXTRACTED KEYWORDS:\n",
            "tariffs, court, appeals court, Donald Trump, Trump, President Donald, Supreme Court, President, impose tariffs, appeals\n",
            "\n",
            "✅ MOST RELATED ARTICLE (from our database):\n",
            "   Title: Global Markets Rally on Tech Sector Growth (Similarity Score: 0.03)\n",
            "=============================================\n",
            "\n",
            "\n",
            "--- ROUGE Evaluation ---\n",
            "Generated Summary: US Court of Appeals for the Federal Circuit rules tariffs are illegal. Ruling will not take effect until 14 October, to give the administration time to ask the US Supreme Court to take up the case. The ruling affects Trump's so-called \"reciprocal\" tariffs, imposed on most countries around the world, as well as other tariffs slapped on China, Mexico and Canada.\n",
            "Reference Summary: A new AI model can analyze medical images more accurately than human radiologists, helping doctors diagnose diseases earlier. This technology, trained on millions of images, is expected to improve patient care and accelerate drug development.\n",
            "\n",
            "Scores:\n",
            "  rouge1: Precision=0.0635, Recall=0.1143, F-measure=0.0816\n",
            "  rouge2: Precision=0.0000, Recall=0.0000, F-measure=0.0000\n",
            "  rougeL: Precision=0.0476, Recall=0.0857, F-measure=0.0612\n",
            "------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the model and tokenizer (this only happens once).\n",
        "    tokenizer, model = initialize_model()\n",
        "\n",
        "    # --- Mock Database of Articles for \"Related Articles\" functionality ---\n",
        "    # In a real application, this would be a large database or search index.\n",
        "    articles_database = [\n",
        "        {\n",
        "            \"title\": \"Global Markets Rally on Tech Sector Growth\",\n",
        "            \"text\": \"Stock markets around the world saw a significant surge today, largely driven by strong quarterly earnings reports from major technology companies. The tech-heavy NASDAQ composite index led the gains, closing up 3%. Investors are optimistic about the future of innovation and digital transformation.\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"New Study Reveals Benefits of a Four-Day Work Week\",\n",
        "            \"text\": \"A landmark study involving 50 companies has found that a four-day work week leads to increased productivity, higher employee satisfaction, and reduced burnout. Many participating companies have decided to make the policy permanent after seeing positive results in both performance and well-being.\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Breakthrough in AI-Powered Drug Discovery\",\n",
        "            \"text\": \"Researchers have developed a new artificial intelligence system that can predict the structure of proteins with unprecedented accuracy. This breakthrough is expected to dramatically accelerate the process of drug discovery and development, potentially leading to new treatments for a wide range of diseases.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # --- Example Usage ---\n",
        "    # The user can choose to enter a URL or paste article text.\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1. Enter a URL to fetch the article\")\n",
        "    print(\"2. Paste article text manually\")\n",
        "    choice = input(\"Enter \\\"1\\\" (for Text)\\n or \\\"2\\\" (for URL): \").strip()\n",
        "\n",
        "    article_text_to_process = None\n",
        "\n",
        "    if choice == '1':\n",
        "        article_text_to_process = input(\"Please paste your article text here and press Enter (or Shift+Enter):\\n\")\n",
        "        \n",
        "    elif choice == '2':\n",
        "        url = input(\"Please enter the article URL: \").strip()\n",
        "        article_text_to_process = get_article_text(url)\n",
        "        if not article_text_to_process:\n",
        "            print(\"Failed to fetch or extract article text from the URL.\")\n",
        "            exit()\n",
        "    else:\n",
        "        print(\"Invalid choice. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    print(f\"\\n--- Processing Article ---\")\n",
        "    print(f\"Article Text: '{article_text_to_process[:500]}...'\")\n",
        "\n",
        "    # 1. Generate the summary\n",
        "    generated_summary = summarize_text(tokenizer, model, article_text_to_process)\n",
        "\n",
        "    # 2. Extract keywords\n",
        "    keywords = extract_keywords(article_text_to_process)\n",
        "\n",
        "    # 3. Find related articles from our database\n",
        "    related_article, score = find_related_articles(article_text_to_process, articles_database)\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(\"\\n\\n=============================================\")\n",
        "    print(\"          ANALYSIS COMPLETE\")\n",
        "    print(\"=============================================\")\n",
        "    print(\"\\n✅ GENERATED SUMMARY:\")\n",
        "    print(generated_summary)\n",
        "    print(\"\\n✅ EXTRACTED KEYWORDS:\")\n",
        "    print(\", \".join(keywords))\n",
        "    if related_article:\n",
        "        print(f\"\\n✅ MOST RELATED ARTICLE (from our database):\")\n",
        "        print(f\"   Title: {related_article['title']} (Similarity Score: {score:.2f})\")\n",
        "    print(\"=============================================\\n\")\n",
        "\n",
        "    # --- Example of ROUGE Evaluation ---\n",
        "    # To evaluate, you need a \"gold standard\" or reference summary.\n",
        "    # Let's create one for our example article.\n",
        "    reference_summary_for_evaluation = \"A new AI model can analyze medical images more accurately than human radiologists, helping doctors diagnose diseases earlier. This technology, trained on millions of images, is expected to improve patient care and accelerate drug development.\"\n",
        "    evaluate_summary(generated_summary, reference_summary_for_evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model saved to ./saved_bart_model\n"
          ]
        }
      ],
      "source": [
        "# Save locally\n",
        "save_directory = \"./saved_bart_model\"\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"✅ Model saved to {save_directory}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
